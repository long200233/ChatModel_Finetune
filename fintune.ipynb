{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Gryphe/MythoMist-7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Gryphe/MythoMist-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=r\"data\\arthur_lora.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3885745093.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    access_token =\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "access_token = \"your_token\"\n",
    "dataset = load_dataset(\"DrAgOn200233/ArthurHayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2324\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2324\n",
      "    })\n",
      "})\n",
      "{'text': '<|im_start|>system\\nAnswer as Arthur Hayes\\n<|im_end|>\\n<|im_start|>assistant\\n\"Oh baby … From junk to bankrupt, that’s the future. And then more money printer go brrrr $BTC = $1mm\\n<|im_end|>'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!autotrain setup --colab > setup_logs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ---\n",
    "#@markdown #### Project Config\n",
    "#@markdown Note: if you are using a restricted/private model, you need to enter your Hugging Face token in the next step.\n",
    "project_name = 'try-finetune' # @param {type:\"string\"}\n",
    "model_name = 'teknium/OpenHermes-2.5-Mistral-7B' # @param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Push to Hub?\n",
    "#@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n",
    "#@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n",
    "#@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n",
    "#@markdown You can find your token here: https://huggingface.co/settings/tokens\n",
    "push_to_hub = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "# hf_token = \"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\" #@param {type:\"string\"}\n",
    "# repo_id = \"Arthur-tweet-openhermes-lora\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Hyperparameters\n",
    "learning_rate = 2e-4 # @param {type:\"number\"}\n",
    "num_epochs = 3 #@param {type:\"number\"}\n",
    "batch_size = 12 # @param {type:\"slider\", min:1, max:32, step:1}\n",
    "block_size = 1024 # @param {type:\"number\"}\n",
    "trainer = \"sft\" # @param [\"default\", \"sft\"] {type:\"raw\"}\n",
    "warmup_ratio = 0.1 # @param {type:\"number\"}\n",
    "weight_decay = 0.01 # @param {type:\"number\"}\n",
    "gradient_accumulation = 4 # @param {type:\"number\"}\n",
    "mixed_precision = \"fp16\" # @param [\"fp16\", \"bf16\", \"none\"] {type:\"raw\"}\n",
    "peft = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "quantization = \"int4\" # @param [\"int4\", \"int8\", \"none\"] {type:\"raw\"}\n",
    "lora_r = 16 #@param {type:\"number\"}\n",
    "lora_alpha = 32 #@param {type:\"number\"}\n",
    "lora_dropout = 0.05 #@param {type:\"number\"}\n",
    "\n",
    "os.environ[\"PROJECT_NAME\"] = project_name\n",
    "os.environ[\"MODEL_NAME\"] = model_name\n",
    "os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n",
    "# os.environ[\"HF_TOKEN\"] = hf_token\n",
    "# os.environ[\"REPO_ID\"] = repo_id\n",
    "os.environ[\"TRAINER\"] = trainer\n",
    "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
    "os.environ[\"NUM_EPOCHS\"] = str(num_epochs)\n",
    "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
    "os.environ[\"BLOCK_SIZE\"] = str(block_size)\n",
    "os.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\n",
    "os.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\n",
    "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
    "os.environ[\"MIXED_PRECISION\"] = str(mixed_precision)\n",
    "os.environ[\"PEFT\"] = str(peft)\n",
    "os.environ[\"QUANTIZATION\"] = str(quantization)\n",
    "os.environ[\"LORA_R\"] = str(lora_r)\n",
    "os.environ[\"LORA_ALPHA\"] = str(lora_alpha)\n",
    "os.environ[\"LORA_DROPOUT\"] = str(lora_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environement Variable Format:\n",
    "# 1. Windows: %var%\n",
    "# 2. Linux: $var\n",
    "\n",
    "cmd = \"\"\"autotrain llm --train \\\n",
    "--model %MODEL_NAME% \\\n",
    "--project-name %PROJECT_NAME% \\\n",
    "--data-path ./data/ \\\n",
    "--text-column text \\\n",
    "--lr %LEARNING_RATE% \\\n",
    "--batch-size %BATCH_SIZE% \\\n",
    "--epochs %NUM_EPOCHS% \\\n",
    "--block-size %BLOCK_SIZE% \\\n",
    "--warmup-ratio %WARMUP_RATIO% \\\n",
    "--lora-r %LORA_R% \\\n",
    "--lora-alpha %LORA_ALPHA% \\\n",
    "--lora-dropout %LORA_DROPOUT% \\\n",
    "--weight-decay %WEIGHT_DECAY% \\\n",
    "--gradient-accumulation %GRADIENT_ACCUMULATION% \\\n",
    "--quantization %QUANTIZATION% \\\n",
    "--mixed-precision %MIXED_PRECISION%\n",
    "--trainer %TRAINER% \\\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
